{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "polygonProject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNiA7MVEnNGtJzc98aLOquW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indl1670/KST_Project/blob/maskrcnn/polygonProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RU6B1IbJpRAT",
        "outputId": "9c3eef79-9f93-4f75-d49f-ce093aa3a0ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/kst_project/side/maskrcnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuwMcVaLpm4h",
        "outputId": "7312e332-3e6b-4355-da31-9dbf302190a9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/kst_project/side/maskrcnn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "# json 파일 경로 지정\n",
        "path = input(\"Input path: \")\n",
        "\n",
        "# json 파일 경로 리스트에 저장\n",
        "file_list = os.listdir(path)\n",
        "file_list_json = [file for file in file_list if file.endswith(\".json\")]\n",
        "\n",
        "trn_img = []\n",
        "\n",
        "# json 파일 전체 탐색\n",
        "for i in range(len(file_list_json)):\n",
        "    img_name = []\n",
        "\n",
        "    # json 파일 로드\n",
        "    file_path =  path + \"/\" + file_list_json[i]\n",
        "    with open(file_path, 'r', encoding='UTF-8') as f:\n",
        "        data = json.load(f)\n",
        "        \n",
        "        \n",
        "        # 라벨링이 있는 이미지 파일 명 저장\n",
        "        for e in data['images']:\n",
        "            for f in data['annotations']:\n",
        "                if e['id'] == f['image_id']:\n",
        "                    if f['segmentation'] != []:\n",
        "                        img_name.append(e['file_name'])\n",
        "        img_name = set(img_name)\n",
        "        \n",
        "    trn_img.extend(img_name)\n",
        "\n",
        "# 해당 이미지 분리: 총 16031     \n",
        "print(trn_img)"
      ],
      "metadata": {
        "id": "xXdRvWjwppeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def main():\n",
        "    json_dir = input('json폴더의 주소: ')\n",
        "\n",
        "    json_list = os.listdir(json_dir)\n",
        "\n",
        "    if not os.path.exists(json_dir + '//train'):\n",
        "        os.makedirs(json_dir + '//train')\n",
        "    if not os.path.exists(json_dir + '//val'):\n",
        "        os.makedirs(json_dir + '//val')\n",
        "    if not os.path.exists(json_dir + '//test'):\n",
        "        os.makedirs(json_dir + '//test')\n",
        "\n",
        "    # train:valid:test = 7:2:1\n",
        "    train_set, test_set = train_test_split(json_list, test_size=0.3, random_state=123)\n",
        "    valid_set, test_set = train_test_split(test_set, test_size=0.4, random_state=123)\n",
        "\n",
        "    # json 파일을 이동\n",
        "    for tr in train_set:\n",
        "        shutil.copy(os.path.join(json_dir, tr), json_dir + '//train//' + tr)\n",
        "    for va in valid_set:\n",
        "        shutil.copy(os.path.join(json_dir, va), json_dir + '//val//' + va)\n",
        "    for te in test_set:\n",
        "        shutil.copy(os.path.join(json_dir, te), json_dir + '//test//' + te)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODZVXWIOsNGz",
        "outputId": "ab4f343c-59e7-4f0a-a332-245cc883625a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "json폴더의 주소: /content/drive/Shareddrives/KST_Project/JSON\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyodi"
      ],
      "metadata": {
        "id": "RRMf8KcNtUpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "\n",
        "def coco_merge(json_dir, dest, cls):\n",
        "    json_list = os.listdir(json_dir)\n",
        "    file_list_json = [file for file in json_list if file.endswith(\".json\")]\n",
        "\n",
        "    theone = os.path.join(json_dir, file_list_json[0])\n",
        "    thetwo = os.path.join(json_dir, file_list_json[1])\n",
        "    cls = cls + '.json'\n",
        "    target = os.path.join(dest, 'annotations', cls)\n",
        "\n",
        "    os.system('pyodi coco merge %s %s %s' % (theone, thetwo, target))\n",
        "    els = file_list_json[2:]\n",
        "\n",
        "    for e in els:\n",
        "        sac = os.path.join(json_dir, e)\n",
        "        os.system('pyodi coco merge %s %s %s' % (target, sac, target))\n",
        "\n",
        "\n",
        "def image_split(dest, data_pick):\n",
        "    img_dir = input('image 파일들의 경로:')\n",
        "\n",
        "    ann_dir = os.path.join(dest, 'annotations')\n",
        "    img_dest = os.path.join(dest, 'images')\n",
        "\n",
        "    with open(os.path.join(ann_dir, 'train.json'), 'r', encoding='utf8') as ann:\n",
        "        data = json.load(ann)\n",
        "        img_list = data['images']\n",
        "        for img in img_list:\n",
        "            fn = img['file_name']\n",
        "            if fn in data_pick:\n",
        "              if os.path.isfile(os.path.join(img_dir, fn)):\n",
        "                shutil.copy(os.path.join(img_dir, fn), os.path.join(img_dest, 'train'))\n",
        "        print('train finish')\n",
        "\n",
        "    with open(os.path.join(ann_dir, 'val.json'), 'r', encoding='utf8') as ann:\n",
        "        data = json.load(ann)\n",
        "        img_list = data['images']\n",
        "        for img in img_list:\n",
        "            fn = img['file_name']\n",
        "            if fn in data_pick:\n",
        "              if os.path.isfile(os.path.join(img_dir, fn)):\n",
        "                shutil.copy(os.path.join(img_dir, fn), os.path.join(img_dest, 'val'))\n",
        "        print('val finish')\n",
        "\n",
        "    with open(os.path.join(ann_dir, 'test.json'), 'r', encoding='utf8') as ann:\n",
        "        data = json.load(ann)\n",
        "        img_list = data['images']\n",
        "        for img in img_list:\n",
        "            fn = img['file_name']\n",
        "            if fn in data_pick:\n",
        "              if os.path.isfile(os.path.join(img_dir, fn)):\n",
        "                shutil.copy(os.path.join(img_dir, fn), os.path.join(img_dest, 'test'))\n",
        "        print('test finish')\n",
        "\n",
        "\n",
        "def main():\n",
        "    print('--coco data merge--')\n",
        "    train_json = input('train json 파일들의 경로: ')\n",
        "    val_json = input('valid json 파일들의 경로: ')\n",
        "    test_json = input('test json 파일들의 경로: ')\n",
        "\n",
        "    dest = input('split한 데이터셋을 저장할 경로: ')\n",
        "\n",
        "    # 폴더 생성\n",
        "    if not os.path.exists(dest + '//images'):\n",
        "        os.makedirs(dest + '//images')\n",
        "        os.makedirs(dest + '//images//train')\n",
        "        os.makedirs(dest + '//images//val')\n",
        "        os.makedirs(dest + '//images//test')\n",
        "    if not os.path.exists(dest + '//annotations'):\n",
        "        os.makedirs(dest + '//annotations')\n",
        "\n",
        "    # # coco를 train/val/test 별로 합치기\n",
        "    coco_merge(train_json, dest, 'train')\n",
        "    coco_merge(val_json, dest, 'val')\n",
        "    coco_merge(test_json, dest, 'test')\n",
        "\n",
        "    # 원하는 이미지 리스트 넣기 = 위 1.2에서 나온 trn_img를 사용\n",
        "    data_pick = trn_img\n",
        "\n",
        "    print('--image split--')\n",
        "    image_split(dest, data_pick)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GwS1lNyrwKW",
        "outputId": "9abff115-ebfa-4535-a33c-ebc85b07f35a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--coco data merge--\n",
            "train json 파일들의 경로: /content/drive/Shareddrives/KST_Project/JSON/train\n",
            "valid json 파일들의 경로: /content/drive/Shareddrives/KST_Project/JSON/val\n",
            "test json 파일들의 경로: /content/drive/Shareddrives/KST_Project/JSON/test\n",
            "split한 데이터셋을 저장할 경로: /content/drive/Shareddrives/KST_Project/polygon\n",
            "--image split--\n",
            "train finish\n",
            "val finish\n",
            "test finish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "dir = '/content/drive/Shareddrives/KST_Project/polygon/annotations/test.json'\n",
        "\n",
        "f = open(dir, 'r', encoding = 'utf8')\n",
        "data = json.load(f)\n",
        "f.close()\n",
        "with open(dir, 'w', encoding='utf8') as w:\n",
        "  for l in data['annotations']:\n",
        "    if l['category_id'] == 1:\n",
        "      l['category_id'] = 6\n",
        "    elif l['category_id'] == 2:\n",
        "      l['category_id'] = 5\n",
        "    elif l['category_id'] == 3:\n",
        "      l['category_id'] = 4\n",
        "    elif l['category_id'] == 4:\n",
        "      l['category_id'] = 3\n",
        "    elif l['category_id'] == 5:\n",
        "      l['category_id'] = 2\n",
        "    elif l['category_id'] == 6:\n",
        "      l['category_id'] = 1\n",
        "  json.dump(data, w, indent='\\t')"
      ],
      "metadata": {
        "id": "s2qAbbrRPhF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "json_dir = '/content/drive/Shareddrives/KST_Project/polygon/annotations'\n",
        "img_dir = '/content/drive/Shareddrives/KST_Project/polygon/images'\n",
        "cls = ['train', 'val', 'test']\n",
        "\n",
        "for c in cls:\n",
        "  cnt = 0\n",
        "  img_name_list = os.listdir(os.path.join(img_dir, c))\n",
        "  with open(os.path.join(json_dir, c+'.json'), 'r', encoding='utf8') as js:\n",
        "    data = json.load(js)\n",
        "    img_list = data['images']\n",
        "    for img in img_list:\n",
        "      if img['file_name'] in img_name_list:\n",
        "        cnt+=1\n",
        "    print(c, 'image: ', len(img_name_list))\n",
        "    print(c, 'image from json: ', len(img_list))\n",
        "    print(c, 'match image: ', cnt)\n"
      ],
      "metadata": {
        "id": "vEk2l7E4mFFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "metadata": {
        "id": "rVulSRTjbM6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/kst_project/oneCycle/mmdetection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKxR7zn9t8d0",
        "outputId": "91ffc4f4-8403-418f-f52a-17f4f95c84e6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/kst_project/oneCycle/mmdetection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"Torch version:{}\".format(torch.__version__))\n",
        "print(\"cuda version: {}\".format(torch.version.cuda))\n",
        "print(\"cudnn version:{}\".format(torch.backends.cudnn.version()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRv10wwqs3Lc",
        "outputId": "ef259049-33e4-4595-dc29-6a996102887f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version:1.10.0+cu111\n",
            "cuda version: 11.1\n",
            "cudnn version:8005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openmim\n",
        "!mim install mmdet==2.19.0\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu111/torch1.10.0/index.html"
      ],
      "metadata": {
        "id": "uI6Vm5e6heOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --branch v2.19.0 https://github.com/open-mmlab/mmdetection.git"
      ],
      "metadata": {
        "id": "prWmnt8iuAdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements/build.txt\n",
        "!python setup.py develop"
      ],
      "metadata": {
        "id": "dyJaqeghujPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for instaboost\n",
        "!pip install instaboostfast\n",
        "# for panoptic segmentation\n",
        "!pip install git+https://github.com/cocodataset/panopticapi.git\n",
        "# for LVIS dataset\n",
        "!pip install git+https://github.com/lvis-dataset/lvis-api.git\n",
        "# for albumentations\n",
        "!pip install albumentations>=0.3.2 --no-binary imgaug,albumentations"
      ],
      "metadata": {
        "id": "jXCiWXoX5yrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python tools/train.py configs/_base_/default_runtime.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r8WP4dpBIyo",
        "outputId": "c04f236b-8228-411e-dc94-f5f572599d53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-19 14:32:10,453 - mmdet - INFO - Environment info:\n",
            "------------------------------------------------------------\n",
            "sys.platform: linux\n",
            "Python: 3.7.12 (default, Sep 10 2021, 00:21:48) [GCC 7.5.0]\n",
            "CUDA available: True\n",
            "GPU 0: Tesla K80\n",
            "CUDA_HOME: /usr/local/cuda\n",
            "NVCC: Build cuda_11.1.TC455_06.29190527_0\n",
            "GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0\n",
            "PyTorch: 1.10.0+cu111\n",
            "PyTorch compiling details: PyTorch built with:\n",
            "  - GCC 7.3\n",
            "  - C++ Version: 201402\n",
            "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
            "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
            "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
            "  - LAPACK is enabled (usually provided by MKL)\n",
            "  - NNPACK is enabled\n",
            "  - CPU capability usage: AVX2\n",
            "  - CUDA Runtime 11.1\n",
            "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
            "  - CuDNN 8.0.5\n",
            "  - Magma 2.5.2\n",
            "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.1, CUDNN_VERSION=8.0.5, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
            "\n",
            "TorchVision: 0.11.1+cu111\n",
            "OpenCV: 4.1.2\n",
            "MMCV: 1.4.1\n",
            "MMCV Compiler: GCC 7.3\n",
            "MMCV CUDA Compiler: 11.1\n",
            "MMDetection: 2.19.0+f08548b\n",
            "------------------------------------------------------------\n",
            "\n",
            "2021-12-19 14:32:11,581 - mmdet - INFO - Distributed training: False\n",
            "2021-12-19 14:32:12,898 - mmdet - INFO - Config:\n",
            "model = dict(\n",
            "    type='MaskRCNN',\n",
            "    pretrained='torchvision://resnet50',\n",
            "    backbone=dict(\n",
            "        type='ResNet',\n",
            "        depth=50,\n",
            "        num_stages=4,\n",
            "        out_indices=(0, 1, 2, 3),\n",
            "        frozen_stages=1,\n",
            "        norm_cfg=dict(type='BN', requires_grad=True),\n",
            "        norm_eval=True,\n",
            "        style='pytorch'),\n",
            "    neck=dict(\n",
            "        type='FPN',\n",
            "        in_channels=[256, 512, 1024, 2048],\n",
            "        out_channels=256,\n",
            "        num_outs=5),\n",
            "    rpn_head=dict(\n",
            "        type='RPNHead',\n",
            "        in_channels=256,\n",
            "        feat_channels=256,\n",
            "        anchor_generator=dict(\n",
            "            type='AnchorGenerator',\n",
            "            scales=[8],\n",
            "            ratios=[0.5, 1.0, 2.0],\n",
            "            strides=[4, 8, 16, 32, 64]),\n",
            "        bbox_coder=dict(\n",
            "            type='DeltaXYWHBBoxCoder',\n",
            "            target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "            target_stds=[1.0, 1.0, 1.0, 1.0]),\n",
            "        loss_cls=dict(\n",
            "            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n",
            "        loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
            "    roi_head=dict(\n",
            "        type='StandardRoIHead',\n",
            "        bbox_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=7, sampling_ratio=0),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        bbox_head=dict(\n",
            "            type='Shared2FCBBoxHead',\n",
            "            in_channels=256,\n",
            "            fc_out_channels=1024,\n",
            "            roi_feat_size=7,\n",
            "            num_classes=6,\n",
            "            bbox_coder=dict(\n",
            "                type='DeltaXYWHBBoxCoder',\n",
            "                target_means=[0.0, 0.0, 0.0, 0.0],\n",
            "                target_stds=[0.1, 0.1, 0.2, 0.2]),\n",
            "            reg_class_agnostic=False,\n",
            "            loss_cls=dict(\n",
            "                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n",
            "            loss_bbox=dict(type='L1Loss', loss_weight=1.0)),\n",
            "        mask_roi_extractor=dict(\n",
            "            type='SingleRoIExtractor',\n",
            "            roi_layer=dict(type='RoIAlign', output_size=14, sampling_ratio=0),\n",
            "            out_channels=256,\n",
            "            featmap_strides=[4, 8, 16, 32]),\n",
            "        mask_head=dict(\n",
            "            type='FCNMaskHead',\n",
            "            num_convs=4,\n",
            "            in_channels=256,\n",
            "            conv_out_channels=256,\n",
            "            num_classes=6,\n",
            "            loss_mask=dict(\n",
            "                type='CrossEntropyLoss', use_mask=True, loss_weight=1.0))))\n",
            "train_cfg = dict(\n",
            "    rpn=dict(\n",
            "        assigner=dict(\n",
            "            type='MaxIoUAssigner',\n",
            "            pos_iou_thr=0.7,\n",
            "            neg_iou_thr=0.3,\n",
            "            min_pos_iou=0.3,\n",
            "            match_low_quality=True,\n",
            "            ignore_iof_thr=-1),\n",
            "        sampler=dict(\n",
            "            type='RandomSampler',\n",
            "            num=256,\n",
            "            pos_fraction=0.5,\n",
            "            neg_pos_ub=-1,\n",
            "            add_gt_as_proposals=False),\n",
            "        allowed_border=-1,\n",
            "        pos_weight=-1,\n",
            "        debug=False),\n",
            "    rpn_proposal=dict(\n",
            "        nms_across_levels=False,\n",
            "        nms_pre=2000,\n",
            "        nms_post=1000,\n",
            "        max_num=1000,\n",
            "        nms_thr=0.7,\n",
            "        min_bbox_size=0),\n",
            "    rcnn=dict(\n",
            "        assigner=dict(\n",
            "            type='MaxIoUAssigner',\n",
            "            pos_iou_thr=0.5,\n",
            "            neg_iou_thr=0.5,\n",
            "            min_pos_iou=0.5,\n",
            "            match_low_quality=True,\n",
            "            ignore_iof_thr=-1),\n",
            "        sampler=dict(\n",
            "            type='RandomSampler',\n",
            "            num=512,\n",
            "            pos_fraction=0.25,\n",
            "            neg_pos_ub=-1,\n",
            "            add_gt_as_proposals=True),\n",
            "        mask_size=28,\n",
            "        pos_weight=-1,\n",
            "        debug=False))\n",
            "test_cfg = dict(\n",
            "    rpn=dict(\n",
            "        nms_across_levels=False,\n",
            "        nms_pre=1000,\n",
            "        nms_post=1000,\n",
            "        max_num=1000,\n",
            "        nms_thr=0.7,\n",
            "        min_bbox_size=0),\n",
            "    rcnn=dict(\n",
            "        score_thr=0.05,\n",
            "        nms=dict(type='nms', iou_threshold=0.3),\n",
            "        max_per_img=100,\n",
            "        mask_thr_binary=0.45))\n",
            "optimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\n",
            "optimizer_config = dict(grad_clip=None)\n",
            "lr_config = dict(\n",
            "    policy='step',\n",
            "    warmup='linear',\n",
            "    warmup_iters=500,\n",
            "    warmup_ratio=0.001,\n",
            "    step=[8, 11])\n",
            "total_epochs = 12\n",
            "dataset_type = 'CocoDataset'\n",
            "data_root = '/content/drive/Shareddrives/KST_Project/polygon/'\n",
            "img_norm_cfg = dict(\n",
            "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
            "train_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
            "    dict(type='Resize', img_scale=(1920, 1080), keep_ratio=True),\n",
            "    dict(type='RandomFlip', flip_ratio=0.5),\n",
            "    dict(\n",
            "        type='Normalize',\n",
            "        mean=[123.675, 116.28, 103.53],\n",
            "        std=[58.395, 57.12, 57.375],\n",
            "        to_rgb=True),\n",
            "    dict(type='Pad', size_divisor=32),\n",
            "    dict(type='DefaultFormatBundle'),\n",
            "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
            "]\n",
            "test_pipeline = [\n",
            "    dict(type='LoadImageFromFile'),\n",
            "    dict(\n",
            "        type='MultiScaleFlipAug',\n",
            "        img_scale=(1920, 1080),\n",
            "        flip=False,\n",
            "        transforms=[\n",
            "            dict(type='Resize', keep_ratio=True),\n",
            "            dict(type='RandomFlip'),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='ImageToTensor', keys=['img']),\n",
            "            dict(type='Collect', keys=['img'])\n",
            "        ])\n",
            "]\n",
            "data = dict(\n",
            "    samples_per_gpu=8,\n",
            "    workers_per_gpu=2,\n",
            "    train=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file=\n",
            "        '/content/drive/Shareddrives/KST_Project/polygon/annotations/train.json',\n",
            "        img_prefix=\n",
            "        '/content/drive/Shareddrives/KST_Project/polygon/images/train',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(type='LoadAnnotations', with_bbox=True, with_mask=True),\n",
            "            dict(type='Resize', img_scale=(1920, 1080), keep_ratio=True),\n",
            "            dict(type='RandomFlip', flip_ratio=0.5),\n",
            "            dict(\n",
            "                type='Normalize',\n",
            "                mean=[123.675, 116.28, 103.53],\n",
            "                std=[58.395, 57.12, 57.375],\n",
            "                to_rgb=True),\n",
            "            dict(type='Pad', size_divisor=32),\n",
            "            dict(type='DefaultFormatBundle'),\n",
            "            dict(\n",
            "                type='Collect',\n",
            "                keys=['img', 'gt_bboxes', 'gt_labels', 'gt_masks'])\n",
            "        ]),\n",
            "    val=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file=\n",
            "        '/content/drive/Shareddrives/KST_Project/polygon/annotations/val.json',\n",
            "        img_prefix='/content/drive/Shareddrives/KST_Project/polygon/images/val',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1920, 1080),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ]),\n",
            "    test=dict(\n",
            "        type='CocoDataset',\n",
            "        ann_file=\n",
            "        '/content/drive/Shareddrives/KST_Project/polygon/annotations/test.json',\n",
            "        img_prefix=\n",
            "        '/content/drive/Shareddrives/KST_Project/polygon/images/test',\n",
            "        pipeline=[\n",
            "            dict(type='LoadImageFromFile'),\n",
            "            dict(\n",
            "                type='MultiScaleFlipAug',\n",
            "                img_scale=(1920, 1080),\n",
            "                flip=False,\n",
            "                transforms=[\n",
            "                    dict(type='Resize', keep_ratio=True),\n",
            "                    dict(type='RandomFlip'),\n",
            "                    dict(\n",
            "                        type='Normalize',\n",
            "                        mean=[123.675, 116.28, 103.53],\n",
            "                        std=[58.395, 57.12, 57.375],\n",
            "                        to_rgb=True),\n",
            "                    dict(type='Pad', size_divisor=32),\n",
            "                    dict(type='ImageToTensor', keys=['img']),\n",
            "                    dict(type='Collect', keys=['img'])\n",
            "                ])\n",
            "        ]))\n",
            "evaluation = dict(metric=['bbox', 'segm'])\n",
            "checkpoint_config = dict(interval=1)\n",
            "log_config = dict(interval=50, hooks=[dict(type='TextLoggerHook')])\n",
            "dist_params = dict(backend='nccl')\n",
            "log_level = 'INFO'\n",
            "load_from = None\n",
            "resume_from = None\n",
            "workflow = [('train', 1)]\n",
            "work_dir = './work_dirs/default_runtime'\n",
            "gpu_ids = range(0, 1)\n",
            "\n",
            "2021-12-19 14:32:13,143 - mmdet - INFO - Set random seed to 1512587911, deterministic: False\n",
            "/usr/local/lib/python3.7/dist-packages/mmdet/models/builder.py:53: UserWarning: train_cfg and test_cfg is deprecated, please specify them in model\n",
            "  'please specify them in model', UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/mmdet/models/detectors/two_stage.py:29: UserWarning: DeprecationWarning: pretrained is deprecated, please use \"init_cfg\" instead\n",
            "  warnings.warn('DeprecationWarning: pretrained is deprecated, '\n",
            "/usr/local/lib/python3.7/dist-packages/mmdet/models/backbones/resnet.py:401: UserWarning: DeprecationWarning: pretrained is deprecated, please use \"init_cfg\" instead\n",
            "  warnings.warn('DeprecationWarning: pretrained is deprecated, '\n",
            "2021-12-19 14:32:13,639 - mmdet - INFO - initialize ResNet with init_cfg {'type': 'Pretrained', 'checkpoint': 'torchvision://resnet50'}\n",
            "2021-12-19 14:32:13,639 - mmcv - INFO - load model from: torchvision://resnet50\n",
            "2021-12-19 14:32:13,639 - mmcv - INFO - load checkpoint from torchvision path: torchvision://resnet50\n",
            "2021-12-19 14:32:13,756 - mmcv - WARNING - The model and loaded state dict do not match exactly\n",
            "\n",
            "unexpected key in source state_dict: fc.weight, fc.bias\n",
            "\n",
            "2021-12-19 14:32:13,792 - mmdet - INFO - initialize FPN with init_cfg {'type': 'Xavier', 'layer': 'Conv2d', 'distribution': 'uniform'}\n",
            "2021-12-19 14:32:13,831 - mmdet - INFO - initialize RPNHead with init_cfg {'type': 'Normal', 'layer': 'Conv2d', 'std': 0.01}\n",
            "2021-12-19 14:32:13,837 - mmdet - INFO - initialize Shared2FCBBoxHead with init_cfg [{'type': 'Normal', 'std': 0.01, 'override': {'name': 'fc_cls'}}, {'type': 'Normal', 'std': 0.001, 'override': {'name': 'fc_reg'}}, {'type': 'Xavier', 'override': [{'name': 'shared_fcs'}, {'name': 'cls_fcs'}, {'name': 'reg_fcs'}]}]\n",
            "loading annotations into memory...\n",
            "Done (t=0.72s)\n",
            "creating index...\n",
            "index created!\n",
            "/usr/local/lib/python3.7/dist-packages/mmdet/apis/train.py:136: UserWarning: config is now expected to have a `runner` section, please set `runner` in your config.\n",
            "  'please set `runner` in your config.', UserWarning)\n",
            "loading annotations into memory...\n",
            "Done (t=0.17s)\n",
            "creating index...\n",
            "index created!\n",
            "2021-12-19 14:32:17,984 - mmdet - INFO - Start running, host: root@c3aaaefd9983, work_dir: /content/drive/My Drive/kst_project/oneCycle/mmdetection/work_dirs/default_runtime\n",
            "2021-12-19 14:32:17,985 - mmdet - INFO - Hooks will be executed in the following order:\n",
            "before_run:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_epoch:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_train_iter:\n",
            "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            " -------------------- \n",
            "after_train_iter:\n",
            "(ABOVE_NORMAL) OptimizerHook                      \n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) IterTimerHook                      \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_train_epoch:\n",
            "(NORMAL      ) CheckpointHook                     \n",
            "(LOW         ) EvalHook                           \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_epoch:\n",
            "(LOW         ) IterTimerHook                      \n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "before_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_iter:\n",
            "(LOW         ) IterTimerHook                      \n",
            " -------------------- \n",
            "after_val_epoch:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "after_run:\n",
            "(VERY_LOW    ) TextLoggerHook                     \n",
            " -------------------- \n",
            "2021-12-19 14:32:17,986 - mmdet - INFO - workflow: [('train', 1)], max: 12 epochs\n",
            "2021-12-19 14:32:17,986 - mmdet - INFO - Checkpoints will be saved to /content/drive/My Drive/kst_project/oneCycle/mmdetection/work_dirs/default_runtime by HardDiskBackend.\n",
            "Traceback (most recent call last):\n",
            "  File \"tools/train.py\", line 185, in <module>\n",
            "    main()\n",
            "  File \"tools/train.py\", line 181, in main\n",
            "    meta=meta)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mmdet/apis/train.py\", line 203, in train_detector\n",
            "    runner.run(data_loaders, cfg.workflow)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mmcv/runner/epoch_based_runner.py\", line 127, in run\n",
            "    epoch_runner(data_loaders[i], **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mmcv/runner/epoch_based_runner.py\", line 47, in train\n",
            "    for i, data_batch in enumerate(self.data_loader):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 359, in __iter__\n",
            "    return self._get_iterator()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 305, in _get_iterator\n",
            "    return _MultiProcessingDataLoaderIter(self)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 944, in __init__\n",
            "    self._reset(loader, first_iter=True)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 975, in _reset\n",
            "    self._try_put_index()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 1209, in _try_put_index\n",
            "    index = self._next_index()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\", line 512, in _next_index\n",
            "    return next(self._sampler_iter)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/sampler.py\", line 229, in __iter__\n",
            "    for idx in self.sampler:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/mmdet/datasets/samplers/group_sampler.py\", line 36, in __iter__\n",
            "    indices = np.concatenate(indices)\n",
            "  File \"<__array_function__ internals>\", line 6, in concatenate\n",
            "ValueError: need at least one array to concatenate\n"
          ]
        }
      ]
    }
  ]
}